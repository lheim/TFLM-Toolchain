{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H07 Auto Toolchain Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "756420e2eea345d89e56e77a68982a27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=73, description='Loading:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#todo for benchmark looo\n",
    "#and within benchmakr function\n",
    "widgets.IntProgress(\n",
    "    value=73,\n",
    "    min=0,\n",
    "    max=100,\n",
    "    step=1,\n",
    "    description='Loading:',\n",
    "    bar_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    orientation='horizontal'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# widgets - loop variables\n",
    "tfl_model_selections = widgets.SelectMultiple(\n",
    "    options=sorted(glob.glob(f\"TFLite-model/*full.tflite\")),\n",
    "    description='Select model:',\n",
    "    layout=Layout(width='100%', height='200px')\n",
    ")\n",
    "\n",
    "cmsis_selections = widgets.SelectMultiple(\n",
    "    options=[('none', './TFLu_benchmark-model_mbed'), ('cmsis-nn', './TFLu_benchmark-model_mbed_cmsis-nn')],\n",
    "    description='Select cmsis modes:',\n",
    ")\n",
    "\n",
    "\n",
    "fpu_selections = widgets.SelectMultiple(\n",
    "    options=[('FPU disabled',0),('FPU enabled',1)],\n",
    "    description='Select FPU modes:',\n",
    ")\n",
    "\n",
    "compiler_selections = widgets.SelectMultiple(\n",
    "    options=['-Ofast', '-Os'],\n",
    "    description='Select compiler options:',\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiler widgets\n",
    "\n",
    "inferences_slider = widgets.FloatLogSlider(\n",
    "    value=1,\n",
    "    base=10,\n",
    "    min=0, # max exponent of base\n",
    "    max=4, # min exponent of base\n",
    "    step=1, # exponent step\n",
    "    description='No of repetition per inference'\n",
    ")\n",
    "\n",
    "target_selections = widgets.SelectMultiple(\n",
    "    options=['auto', 'NUCLEO_L496ZG', 'NUCLEO_F767ZI', '...'],\n",
    "    description='Select targets:',\n",
    ")\n",
    "\n",
    "arena_size_slider = widgets.IntSlider(\n",
    "    value=50,\n",
    "    min=0,\n",
    "    max=500, \n",
    "    step=50,\n",
    "    description='Arena Size [KiB]'\n",
    ")\n",
    "\n",
    "cycles_selection = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Benchmark in cycles (instead of us)',\n",
    "    indent=False\n",
    ")\n",
    "layers_selection = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Benchmark with layer granularity (instead of a whole inference)',\n",
    "    indent=False\n",
    ")\n",
    "reporting_selection = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description='Report the results of the inference via UART',\n",
    "    indent=False\n",
    ")\n",
    "input_selection = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description='Enabling custom input via UART (required for automated verification)',\n",
    "    indent=False\n",
    ")\n",
    "energy_selection = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Enable custom settings for an energy measurement',\n",
    "    indent=False\n",
    ")\n",
    "\n",
    "baudrate_slider = widgets.FloatLogSlider(\n",
    "    value=10e6,\n",
    "    base=10,\n",
    "    min=4, # max exponent of base\n",
    "    max=6, # min exponent of base\n",
    "    step=1, # exponent step\n",
    "    description='Baudrate'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "columns = ['time','MCU',\n",
    "           'model', 'mbed-dir', 'cmsis-nn', 'arena_size',\n",
    "           'compiler_optimization','FPU_status', 'FLOPs', 'FLOPs_profiler',\n",
    "           'pruned', 'weights', 'activations', 'model_type',\n",
    "           'model_size', 'model_size_reduction', \n",
    "           'model_size_gzip', 'model_size_reduction_gzip', 'binary_size',\n",
    "           'input_details_dtype', 'input_details_shape', \n",
    "           'output_details_dtype', 'output_details_shape',\n",
    "           'tfl_interpreter_accuracy', 'tfl_interpreter_loss_crossentropy', 'tfl_interpreter_loss_meansquared',\n",
    "           'inferences_per_cycle', 'tflu_mcu_benchmark_single',\n",
    "           'tflu_mcu_benchmark_mean','tflu_mcu_benchmark_std', \n",
    "           'tflu_mcu_accuracy', 'tflu_mcu_loss_crossentropy', 'tflu_mcu_loss_meansquared'\n",
    "            'layer',\n",
    "           'layer_latency_mean',\n",
    "           'layer_latency_std',\n",
    "          ]\n",
    "\n",
    "table_energy = pd.DataFrame()\n",
    "table_firmware = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cmsis(mbed_dir):\n",
    "    if 'cmsis' in mbed_dir:\n",
    "        return 'cmsis-nn'\n",
    "    else:\n",
    "        return 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_model_information(filename, series):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*todo*\n",
    "- currently we only support benchmarking with layer granularity\n",
    "- save all the information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmark(RL, target_device, serial_port, baudrate, tfl_model_file, mbed_dir, compiler_flag, fpu_status, \n",
    "                 no_samples = 50):\n",
    "\n",
    "\n",
    "    tfl_model_string = get_tfl_model_string(tfl_model_file)\n",
    "    print(\"Benchmarking:\", tfl_model_string)\n",
    "\n",
    "    model_information = pd.Series(dtype=np.float32)\n",
    "    #model_information = {}\n",
    "    \n",
    "\n",
    "    model_information['time'] = str(datetime.now())\n",
    "    model_information['MCU'] = target_device\n",
    "    model_information['model'] = tfl_model_string\n",
    "\n",
    "    model_information['mbed-dir'] = mbed_dir\n",
    "    model_information['cmsis-nn'] = get_cmsis(mbed_dir)\n",
    "\n",
    "    read_model_information(tfl_model_string, model_information)\n",
    "\n",
    "    if \"Q-mixed\" in tfl_model_string:\n",
    "        model_information['weights'] = 'int8'\n",
    "        model_information['activations'] = 'float32'\n",
    "    elif \"Q-full\" in tfl_model_string:\n",
    "        model_information['weights'] = 'int8'\n",
    "        model_information['activations'] = 'int8'\n",
    "    else:\n",
    "        model_information['weights'] = 'float32'\n",
    "        model_information['activations'] = 'float32'\n",
    "\n",
    "    model_information['model_type'] = 'W-' + model_information['weights'] + '_A-' +model_information['activations']\n",
    "\n",
    "    \n",
    "    \n",
    "    keras_model = tfl_model_string[:tfl_model_string.find('_Q')]\n",
    "    # keras-flops library\n",
    "    model_information['FLOPs'] = keras_flops_get_flops(f'./keras-model/{keras_model}.h5')\n",
    "    # TF Profiler\n",
    "    model_information['FLOPs_profiler'] = profiler_get_flops(f'./keras-model/{keras_model}.h5')\n",
    "\n",
    "  #  input_details, output_details = get_tfl_details(tfl_model_file)\n",
    "\n",
    "    # model_information['input_details'] = input_details\n",
    "    #model_information['input_details_dtype'] = str(input_details[0]['dtype'])\n",
    " #   model_information['input_details_shape'] = str(input_details[0]['shape'])\n",
    "\n",
    "    # model_information['output_details'] = output_details\n",
    "    #model_information['output_details_dtype'] = str(output_details[0]['dtype'])\n",
    " #   model_information['output_details_shape'] = str(output_details[0]['shape'])\n",
    "\n",
    "    # filesize of the model\n",
    "    model_size, reduction = get_tfl_size(tfl_model_file, unquantized_model_file=basic_model_file)\n",
    "    model_information['model_size'] = model_size\n",
    "    model_information['model_size_reduction'] = reduction\n",
    "\n",
    "    model_size, reduction = get_tfl_size(tfl_model_file, unquantized_model_file=basic_model_file, gzip=True)\n",
    "    model_information['model_size_gzip'] = model_size\n",
    "    model_information['model_size_reduction_gzip'] = reduction\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    # accuracy and error\n",
    "    # todo: this might need adjustments when the input and output datatype differs (e.g. when it's quantized)\n",
    "\n",
    "    ## prediction\n",
    "\n",
    "    # only calculate the predictions once for each model - this might take a while\n",
    "#     if tfl_model_file in accuracies:\n",
    "#         # it already exists\n",
    "#         print(\"\\tPredictions from local evaluation already exist. Reusing them.\")\n",
    "#         model_information['tfl_interpreter_accuracy'] = accuracies[tfl_model_file]\n",
    "#     else:\n",
    "#         # we have to do the math\n",
    "#         print(\"\\tRunning local evaluation of model using TF Lite Interpreter ...\")\n",
    "#         tfl_model_predictions = predict(tfl_model_file, x=x_test_normalized)\n",
    "#         accuracies[tfl_model_file] = accuracy(tfl_model_predictions, y=y_test)\n",
    "#         model_information['tfl_interpreter_accuracy'] = accuracies[tfl_model_file]\n",
    "\n",
    "#         print(\"\\tEvaluation finished.\")\n",
    "# \n",
    "# \n",
    "#     ## loss\n",
    "#     model_information['tfl_interpreter_loss_crossentropy'] = loss_fn_crossentropy(y_test, tfl_model_predictions).numpy()\n",
    "#     model_information['tfl_interpreter_loss_meansquared'] = loss_fn_meansquared(y_test, tfl_model_predictions).numpy()\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    tfl_model_to_file(tfl_model_file, mbed_dir)\n",
    "\n",
    "    write_constants(tfl_model_string, 1, image_no, mbed_dir)\n",
    "\n",
    "    set_compiler_flag(mbed_dir, compiler_flag)\n",
    "    model_information['compiler_optimization'] = compiler_flag\n",
    "\n",
    "    toggle_fpu(mbed_dir, fpu_status)\n",
    "    model_information['FPU_status'] = fpu_status\n",
    "\n",
    "    print(\"\\tBuilding & flashing ...\")\n",
    "    !cd {mbed_dir} && mbed compile -m {target_device} -t GCC_ARM --profile release --flash {arguments} > build_log.txt\n",
    "\n",
    "    if _exit_code != 0:\n",
    "        print(\"\\tThere was an error during compilation. Skipping ...\")\n",
    "        model_information['error'] = 'error during compilation'\n",
    "        return model_information, model_information\n",
    "\n",
    "    print(\"\\tFinished building & flashing.\")\n",
    "    \n",
    "    print(\"\\tGetting size of the binary blob ...\")\n",
    "    binary_filename = glob.glob(f\"{mbed_dir}/BUILD/{target_device}/GCC_ARM-RELEASE/*.bin\")\n",
    "    model_information['binary_size'] = os.path.getsize(binary_filename[0])\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## start rocketlogger \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## TODO: energy meausrmenet, and non layer measurement\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "    port = set_port(serial_port, baudrate)\n",
    "    \n",
    "    ## complete inference\n",
    "#     print(\"\\tVerifying ...\")\n",
    "#         mcu_predictions, benchmark_results = verify_accuracy_on_mcu(port, test=not(run_verification_on_mcu))\n",
    "\n",
    "#     if run_verification_on_mcu:\n",
    "#         model_information['tflu_mcu_benchmark_mean'] = benchmark_results.mean()\n",
    "#         model_information['tflu_mcu_benchmark_std'] = benchmark_results.std()\n",
    "\n",
    "#         model_information['tflu_mcu_accuracy'] = accuracy(mcu_predictions)\n",
    "\n",
    "#         model_information['tflu_mcu_loss_crossentropy'] = loss_fn_crossentropy(y_test, mcu_predictions).numpy()\n",
    "#         model_information['tflu_mcu_loss_meansquared'] = loss_fn_meansquared(y_test, mcu_predictions).numpy()\n",
    "\n",
    "#     else:\n",
    "#         model_information['tflu_mcu_accuracy_50'] = accuracy(mcu_predictions)                    \n",
    "#         model_information['tflu_mcu_benchmark_mean_50'] = benchmark_results.mean()\n",
    "#         model_information['tflu_mcu_benchmark_std_50'] = benchmark_results.std()\n",
    "\n",
    "    \n",
    "    ## Layers\n",
    "    \n",
    "    print(\"\\tFirst emasurmeent which will be discarded - warm up.\")\n",
    "    # discard first result\n",
    "    send_image(port, x_test_normalized[image_no])\n",
    "    read_results(port)\n",
    "    \n",
    "      #  as comment for energy measurement\n",
    "    json_dump = model_information.filter(items=['time', 'MCU', 'model', 'model_type', \n",
    "                                                'cmsis-nn', 'compiler_optimization',\n",
    "                                                'FPU_status']).to_json()\n",
    "    \n",
    "    # json_dump = json.dumps(model_information)\n",
    "    # start energy measurement\n",
    "    \n",
    "    RL.filename = f'{target_device}_{model_name}_{date.today()}'\n",
    "    #RL.comment = str(json_dump)\n",
    "    #print(json_dump)\n",
    "    \n",
    "    RL.set_config()\n",
    "    \n",
    "    RL.start_measurement()\n",
    "    \n",
    "    sample_images = x_test_normalized[image_no:image_no+no_samples]\n",
    "\n",
    "    print(f\"\\tBenchmarking layers (sample of {len(sample_images)} images) ...\")\n",
    "\n",
    "    \n",
    "    table_layers = pd.DataFrame()\n",
    "    for sample_image in sample_images:\n",
    "        send_image(port, sample_image)\n",
    "        _, benchmark_result = read_results(port)\n",
    "        table_layers = pd.concat([table_layers, pd.DataFrame.from_dict(benchmark_result, orient='index')], axis=1)\n",
    "\n",
    "    port.close()\n",
    "    \n",
    "    RL.stop_measurement()\n",
    "    \n",
    "    RL.download_file()\n",
    "    \n",
    "    # process RL file\n",
    "    filename = \"./results/\" + RL.filename + \".rld\" \n",
    "    #df = pd.DataFrame()\n",
    "    #df_layers = pd.DataFrame()\n",
    "    energy_inference_table, energy_inference_layer_table = energy_analysis(filename, model_information)\n",
    "    #df = pd.concat([df, energy_inference_table])\n",
    "    #df_layers = pd.concat([df_layers, energy_inference_layer_table])\n",
    "    \n",
    "\n",
    "    #df.to_excel(f'results/E_{target_device}_{tfl_model_string}_aggregated_energy_inference_table_{date.today()}.xlsx')\n",
    "    #df.to_pickle(f'results/E_{target_device}_{tfl_model_string}_aggregated_energy_inference_table_{date.today()}.pkl')\n",
    "    energy_inference_layer_table.to_excel(f'results/E_{target_device}_{tfl_model_string}_aggregated_energy_inference_layer_table_{date.today()}.xlsx')\n",
    "    energy_inference_layer_table.to_pickle(f'results/E_{target_device}_{tfl_model_string}_aggregated_energy_inference_layer_table_{date.today()}.pkl')\n",
    "    \n",
    "\n",
    "    table_layers.to_pickle(f'results/layers_raw/F_{target_device}_{tfl_model_string}_layer_raw-results_{date.today()}.pkl')\n",
    "    table_layers_summary = pd.DataFrame(columns=columns)\n",
    "\n",
    "    table_layers_summary['layer'] = table_layers.index\n",
    "    table_layers_summary['layer_latency_mean'] = table_layers.mean(axis=1).values\n",
    "    table_layers_summary['layer_latency_std'] = table_layers.std(axis=1).values\n",
    "    #table_layers_summary.loc['sum_latency'] = table_layers_summary.sum()\n",
    "    table_layers_summary.loc[:, 'layer_latency_relativ'] = table_layers_summary.loc[:]['layer_latency_mean'] / table_layers_summary.loc[:]['layer_latency_mean'].sum()    \n",
    "\n",
    "    for key, value in model_information.iteritems():\n",
    "        # print(f\"key: {key}; value: {value}\")\n",
    "        table_layers_summary[key] = value\n",
    "\n",
    "    ## end layers\n",
    "    \n",
    "    #table_layers_summary.to_pickle(f'results_layers/layers_{mbed_dir}_{tfl_model_string}.pkl')\n",
    "    \n",
    "\n",
    "    \n",
    "    return table_layers_summary, energy_inference_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
